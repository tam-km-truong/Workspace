Bacterial data is growing at a breakneck speed, exponentially and quickly out of the capacity of our current infrastructure. To address this problem, a recent innovation by Brinda et al. (Nature Method, 2025), named phylogenetic compression, leverages the evolutionary characteristics of bacterial dna to find an order of genomes in order to improve their compressibility, and achieve lossless compression of bacterial dna of 1-3 order of magnitude. Phylogenetic compression involves 2 key steps, first to cluster the genomes into batches of 4000 using species metadata taxonomy, secondly, for each batch, infer an phylogenetic tree then reorder the genomes in the batch by going from left to right using the treeâ€™s leave nodes. With this approach, the 661k collection from Blackwell et al (2022) has been reduced to 29GB using xz compressor from 805GB just gzipping naively; and the AllTheBacteria collection (2025), from 3TB to just over 100GB.

However, phylogenetic compression has some limitations. Firstly, phylogenetic compression workflow depends on the species level information, the quality of the species metadata could influence the final compression size, and there could be situations where taxonomy information is unavailable or unreliable. Secondly, batching is not well optimized for different use cases, for example, the current batching method generally leads to an imbalance in the post compression sizes; this could make downstream analysis and parallelization inefficient.

Therefore, motivated by the growing scale of bacterial data at public databases such as NCBI and the need for general-purpose, practical solutions that do not require predefined species metadata: How to design an efficient, scalable, and taxonomy-independent method for batching bacterial genomes that improves compression while supporting multiple use cases such as searching, distributed processing and analysis?

The first axis of my research focuses on taxonomy-independent phylogenetic compression.
The goal is to achieve similar levels of compressibility without relying on any predefined species metadata. This is done using a nearest-neighbor strategy based on phylogenetic placement. The approach begins by selecting a representative subset of the genome collection and constructing a skeleton tree, this serves as the basis for the first-stage reordering. The skeleton tree is then used as a referential structure for the rest of the collection. The remaining genomes, referred to as query genomes, are assigned to positions on the skeleton tree using Mash distance as a similarity measure. After this, a second-stage reordering is done: for each group of genomes assigned to the same node or region of the tree, a local phylogenetic tree is inferred and used to reorder the genomes within that group. This two-stage process produces a global phylogenetic order for the entire collection. This approach relies heavily on the quality and representativeness of the initial skeleton tree.

The second axis of my research is about creating a batching strategy that facilitates massively distributed processing and analysis on different computer architecture, as well as improve even further ease of sharing and accessibility of bacterial genomes. The specific goal is to provide a tool for constraint specific batching, such as post compression size upper bound. This is done by looking add the biological characteristic of bacterial genomes such as distinct kmers count and how well is it as an approximation of post-compression size of genomes batches. Combining that correlation with well known optimization problem of bin packing to achieve uniform compressed batch sizes.

The final axis of my research is the tool development and recompression of well known curated collections such as 661k and AllTheBacteria. I have achieved 15% improvement for 661k on the whole collection and an average of 20% for individual species, with just using the skeleton tree technique mentioned in axis 1. The tools I'm developing are aimed at personal computers as well as distributed computer architecture such as computer clusters, Processing in Memory, GPU,...

In conclusion, the state of the art phylogenetic compression, even though powerful, has its share of limitations. With my work, I aim to further improve compression ratio, provide techniques for taxonomy-independent compression, and facilitate downstream analysis on distributed architecture. My side track includes recompression of curated bacterial genomes collections and contributing in developing new tools such as Miniphy2, the second version of phylogenetic compression workflow.
